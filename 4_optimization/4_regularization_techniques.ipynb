{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1b291a",
   "metadata": {},
   "source": [
    "## Regularization Techniques\n",
    "Regularization techniques are methods used to **prevent overfitting** in machine learning models. Overfitting happens when a model learns the training data too well, including noise or random fluctuations, causing poor performance on unseen (test) data. Regularization works by **adding a penalty** or constraint to the model's loss function during training, which discourages overly complex models and helps them generalize better on new data.\n",
    "\n",
    "The penalty is typically a function of the model parameters (weights or coefficients) and is controlled by a **regularization parameter** (often denoted as λ) that balances fitting the training data well and keeping the model simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f5e84",
   "metadata": {},
   "source": [
    "#### L1 Regularization (Lasso Regression)\n",
    "\n",
    "L1 regularization, also called Lasso (Least Absolute Shrinkage and Selection Operator) regression, adds the sum of absolute values of the weights as a penalty term to the loss function:\n",
    "\n",
    "$$\n",
    "J(\\theta) = J(original\\theta) + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - J(θ) is the regularized cost function\n",
    "  - J_original(θ) is the original loss function (e.g., mean squared error)\n",
    "  - λ is the regularization parameter controlling the strength of the penalty\n",
    "  - θ_j are the model parameters (weights)\n",
    "\n",
    "Key Characteristics:\n",
    "- Encourages sparsity: many parameters may be shrunk exactly to zero\n",
    "- Useful for feature selection by effectively removing irrelevant features\n",
    "- Produces simpler, interpretable models\n",
    "- Can cause some coefficients to become zero (feature elimination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e0f0f",
   "metadata": {},
   "source": [
    "#### L2 Regularization (Ridge Regression)\n",
    "\n",
    "L2 regularization, also called Ridge regression, adds the sum of squared values of the weights as a penalty term:\n",
    "\n",
    "$$\n",
    "J(\\theta) = J(original\\theta) + \\lambda \\sum_{j=1}^{n} |\\theta_j|^2\n",
    "$$\n",
    "\n",
    "Key Characteristics:\n",
    "- Shrinks coefficients towards zero but does not force them to be zero\n",
    "- Helps reduce model complexity and improve stability\n",
    "- Useful when many small/medium effects exist rather than sparse features\n",
    "- Produces smaller, more robust weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7addd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularized Cost: 10.75\n",
      "L2 Regularized Cost: 12.525\n"
     ]
    }
   ],
   "source": [
    "# Manual Implementation\n",
    "import numpy as np\n",
    "\n",
    "# Example weights\n",
    "theta = np.array([3, -4, 0.5])\n",
    "lambda_ = 0.1\n",
    "original_cost = 10\n",
    "\n",
    "# L1 Regularization term\n",
    "l1_term = lambda_ * np.sum(np.abs(theta))\n",
    "l1_regularized_cost = original_cost + l1_term\n",
    "\n",
    "# L2 Regularization term\n",
    "l2_term = lambda_ * np.sum(theta ** 2)\n",
    "l2_regularized_cost = original_cost + l2_term\n",
    "\n",
    "print(\"L1 Regularized Cost:\", l1_regularized_cost)\n",
    "print(\"L2 Regularized Cost:\", l2_regularized_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f7457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients: [61.43821843 98.34755587 60.9358236  55.43964642 35.89008021]\n"
     ]
    }
   ],
   "source": [
    "# Using Scikit-Learn\n",
    "# For linear models with built-in regularization:\n",
    "\n",
    "# Lasso Regression (L1):\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)  # alpha corresponds to λ\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Lasso coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629c3d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge coefficients: [61.44968224 98.32085667 60.97256679 55.4690014  35.94653687]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression (L2):\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ridge coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f9652",
   "metadata": {},
   "source": [
    "#### Dropout Regularization\n",
    "\n",
    "Dropout is a regularization technique mainly used in deep learning. During each training iteration:\n",
    "  - Randomly \"drop\" (set to zero) a fraction p of neurons in the network\n",
    "  - This prevents neurons from co-adapting too much\n",
    "  - Forces the network to learn redundant representations and become robust\n",
    "  - Reduces overfitting and improves generalization\n",
    "\n",
    "Key Characteristics:\n",
    "- Only applied during training; during inference, dropout is turned off\n",
    "- Dropout rate p is a hyperparameter (typical values 0.2 to 0.5)\n",
    "- Easy to implement and widely used in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e36199",
   "metadata": {},
   "source": [
    "**Solved Example: Regularized Cost Calculation**\n",
    "\n",
    "Given a linear regression model with three weights θ1 = 3, θ2 = -4, θ3 = 0.5, regularization parameter λ = 0.1, and original cost J_original = 10:\n",
    "\n",
    "L1 Regularization:\n",
    "J(θ) = J_original(θ) + λ Σ_j=1^m |θ_j|\n",
    "\n",
    "J = 10 + 0.1 × (|3| + |-4| + |0.5|)\n",
    "  = 10 + 0.1 × (3 + 4 + 0.5)\n",
    "  = 10 + 0.75\n",
    "  = 10.75\n",
    "\n",
    "L2 Regularization:\n",
    "J(θ) = J_original(θ) + λ Σ_j=1^m θ_j^2\n",
    "\n",
    "J = 10 + 0.1 × (3² + (-4)² + 0.5²)\n",
    "  = 10 + 0.1 × (9 + 16 + 0.25)\n",
    "  = 10 + 2.525\n",
    "  = 12.525\n",
    "\n",
    "Regularization helps generalize ML models by simplifying them. Lasso eliminates some features of the coefficients, while ridge regression reduces their size as the strength of regularization increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bb40b",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Regularization techniques help reduce overfitting and improve generalization of ML models.\n",
    "\n",
    "- L1 regularization (Lasso) encourages sparse models by forcing some weights to zero, useful for feature selection.\n",
    "\n",
    "- L2 regularization (Ridge) shrinks weights but keeps all in the model, stabilizing learning.\n",
    "\n",
    "- Dropout is a robust regularization technique for neural networks, randomly dropping neurons during training.\n",
    "\n",
    "- Python libraries like Scikit-Learn and TensorFlow provide easy-to-use implementations of these techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a22216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://www.ibm.com/think/topics/lasso-regression)\n",
    "# [2](https://www.youtube.com/watch?v=VqKq78PVO9g)\n",
    "# [3](https://builtin.com/data-science/l2-regularization)\n",
    "# [4](https://www.reddit.com/r/deeplearning/comments/17eiu9p/list_here_all_the_regularization_techniques_you/)\n",
    "# [5](https://www.geeksforgeeks.org/machine-learning/regularization-in-machine-learning/)\n",
    "# [6](https://www.e2enetworks.com/blog/regularization-in-deep-learning-l1-l2-dropout)\n",
    "# [7](https://towardsdatascience.com/understanding-l1-and-l2-regularization-93918a5ac8d0/)\n",
    "# [8](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
    "# [9](https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/)\n",
    "# [10](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Understanding-L1-and-L2-regularization-techniques-for-optimized-model-training--Vmlldzo3NzYwNTM5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
