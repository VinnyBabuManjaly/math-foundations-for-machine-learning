{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8f9c5d",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Activation functions are foundational components in neural networks, responsible for deciding the output of each neuron and enabling the network to learn complex, non-linear patterns from data. Without them, neural networks would behave like simple linear models, incapable of solving sophisticated problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b9523",
   "metadata": {},
   "source": [
    "An activation function is a mathematical function applied to the output of a neuron (node) after weighted input summation and bias addition. Its role is to:\n",
    "\n",
    "- Transform the input signal into a form suitable for the next layer.\n",
    "\n",
    "- Add **non-linearity** to the network, allowing it to learn complex mappings beyond linear relationships.\n",
    "\n",
    "- Govern whether a neuron should \"activate\" (produce significant output) or not.\n",
    "\n",
    "This non-linearity is crucial; stacking multiple linear layers without activation reduces to a single linear transformation, limiting expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d26db",
   "metadata": {},
   "source": [
    "#### Sigmoid Activation Function\n",
    "\n",
    "The sigmoid function maps any real-valued input x to a value between 0 and 1. Commonly used for binary classification as it represents probability-like outputs.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Domain: (-∞, ∞)\n",
    "- Range: (0, 1)\n",
    "- S-shaped curve, centered at x = 0, where σ(0) = 0.5\n",
    "- As x → +∞, σ(x) → 1; as x → -∞, σ(x) → 0\n",
    "- Used for output neurons predicting probabilities\n",
    "- Drawbacks: Prone to vanishing gradient problem in deep networks, leading to slow or stalled learning in early layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66b4ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid outputs: [0.11920292 0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example:\n",
    "inputs = np.array([-2, 0, 2])\n",
    "outputs = sigmoid(inputs)\n",
    "print(\"Sigmoid outputs:\", outputs)\n",
    "\n",
    "\n",
    "# Built-in Implementations\n",
    "# TensorFlow: tf.nn.sigmoid\n",
    "# PyTorch: torch.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084716f7",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "ReLU outputs zero for any negative input, and outputs the input directly if positive.\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Range: [0, ∞)\n",
    "- Simple, computationally efficient\n",
    "- Avoids vanishing gradient problem for positive values, enabling faster training of deep networks\n",
    "- Can suffer from dying ReLU problem: neurons stuck outputting zero if inputs remain negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b44990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU outputs: [0 0 2]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example:\n",
    "inputs = np.array([-2, 0, 2])\n",
    "outputs = relu(inputs)\n",
    "print(\"ReLU outputs:\", outputs)\n",
    "\n",
    "# Built-in Implementations\n",
    "# TensorFlow: tf.nn.relu\n",
    "# PyTorch: torch.relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b59615",
   "metadata": {},
   "source": [
    "#### Other Common Activation Functions\n",
    "\n",
    "1. Hyperbolic Tangent (Tanh)\n",
    "- Similar shape to sigmoid but outputs values between -1 and 1\n",
    "\n",
    "        tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "\n",
    "- Centered at zero, often leads to better convergence than sigmoid\n",
    "- Still suffers from vanishing gradients in deep networks\n",
    "\n",
    "2. Leaky ReLU\n",
    "- Variant of ReLU that allows a small, non-zero gradient for negative inputs to mitigate dying ReLU\n",
    "\n",
    "        LeakyReLU(x) = max(αx, x), where α is small (e.g., 0.01)\n",
    "\n",
    "3. Softmax\n",
    "- Used in output layer of multi-class classification\n",
    "- Converts raw logits into probabilities summing to 1\n",
    "- Normalized exponential\n",
    "\n",
    "        Softmax(z_i) = e^(z_i) / Σ_j e^(z_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bc7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://encord.com/blog/activation-functions-neural-networks/)\n",
    "# [2](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "# [3](https://www.geeksforgeeks.org/machine-learning/activation-functions-neural-networks/)\n",
    "# [4](https://towardsdatascience.com/activation-functions-in-neural-networks-how-to-choose-the-right-one-cb20414c04e5/)\n",
    "# [5](https://en.wikipedia.org/wiki/Activation_function)\n",
    "# [6](https://www.reddit.com/r/learnmachinelearning/comments/12j4hxa/understanding_activation_functions_a_mustknow_for/)\n",
    "# [7](https://developers.google.com/machine-learning/crash-course/neural-networks/activation-functions)\n",
    "# [8](https://www.reddit.com/r/MLQuestions/comments/13j1g1y/purpose_of_activation_functions_in_neural_network/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
