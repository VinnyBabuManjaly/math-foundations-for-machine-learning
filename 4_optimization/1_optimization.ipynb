{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61513092",
   "metadata": {},
   "source": [
    "Optimization in machine learning is the fundamental process of adjusting model parameters and architecture to minimize errors and maximize predictive accuracy, ensuring the most effective learning from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a7b3a",
   "metadata": {},
   "source": [
    "### Core Principles\n",
    "**Objective/Loss Function**: Machine learning models are trained to minimize (or maximize) an objective or cost function, such as Mean Squared Error (MSE) for regression or cross-entropy for classification, which quantifies the difference between predicted outputs and actual labels.\n",
    "\n",
    "**Parameter and Hyperparameter Tuning**: Optimization includes learning model parameters (like weights in neural networks) and tuning hyperparameters (like learning rate or regularization strength) to achieve the best generalization and performance.\n",
    "\n",
    "**Iterative Improvement**: Optimization is typically an iterative process in which the model is updated repeatedly based on the evaluation of the objective function, guiding the model toward optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae83c45",
   "metadata": {},
   "source": [
    "**Gradient Descent**: The most common optimization algorithm, which updates parameters in the direction opposite to the gradient of the objective function to reduce cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://www.neuralconcept.com/post/machine-learning-based-optimization-methods-use-cases-for-design-engineers)\n",
    "# [2](https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b/)\n",
    "# [3](https://www.geeksforgeeks.org/machine-learning/optimization-algorithms-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295930a",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm central to machine learning, where it updates model parameters to minimize a loss (error) function by moving in the direction of steepest descent.\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "    θ := θ - η ∇J(θ)\n",
    "\n",
    "where:\n",
    "\n",
    "- θ is the parameter vector (e.g., weights),  \n",
    "- J(θ) is the loss function,  \n",
    "- η is the learning rate,  \n",
    "- ∇J(θ) is the gradient of the loss function with respect to θ.  \n",
    "\n",
    "At each step, the algorithm moves the parameter θ closer to the minimum of J(θ) by subtracting a fraction (step size η) of the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d19f8",
   "metadata": {},
   "source": [
    "#### Local Minimum vs. Global Minimum\n",
    "A **local minimum** is a point where the loss is lower than adjacent points but not necessarily the lowest overall.\n",
    "\n",
    "The **global minimum** is the absolute lowest loss value possible.\n",
    "\n",
    "For convex loss functions, gradient descent is guaranteed to reach the global minimum, but for non-convex functions (like neural nets), it can get stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef0232",
   "metadata": {},
   "source": [
    "#### Learning Rate (η)\n",
    "\n",
    "The step size η determines how far θ moves in each iteration.\n",
    "- If η is too large, updates may overshoot the minimum and potentially diverge.\n",
    "- If η is too small, convergence will be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518709e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: θ = 0.6000000000000001\n",
      "Iteration 2: θ = 1.08\n",
      "Iteration 3: θ = 1.464\n",
      "Iteration 4: θ = 1.7711999999999999\n",
      "Iteration 5: θ = 2.01696\n",
      "Iteration 6: θ = 2.213568\n",
      "Iteration 7: θ = 2.3708544\n",
      "Iteration 8: θ = 2.49668352\n",
      "Iteration 9: θ = 2.597346816\n",
      "Iteration 10: θ = 2.6778774528\n"
     ]
    }
   ],
   "source": [
    "# Minimal Gradient Descent Example for J(θ) = (θ - 3)^2\n",
    "\n",
    "# Initialize parameters\n",
    "theta = 0.0        # Starting value of θ\n",
    "eta = 0.1        # Learning rate\n",
    "num_iterations = 10\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for i in range(num_iterations):\n",
    "    grad = 2 * (theta - 3)      # Derivative of J(θ) = (θ - 3)^2\n",
    "    theta = theta - eta * grad  # Update θ\n",
    "    print(f\"Iteration {i+1}: θ = {theta}\")\n",
    "\n",
    "# Each iteration computes the current loss, gradient, and updates θ along the \n",
    "# negative gradient direction, moving toward the minimum at θ=3.\n",
    "# i.e. Each step moves the value of θ closer to the minimum of J(θ) (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad5b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://www.geeksforgeeks.org/machine-learning/gradient-descent-algorithm-and-its-variants/)\n",
    "# [2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent)\n",
    "# [3](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "# [4](https://opus.govst.edu/cgi/viewcontent.cgi?article=1001&context=theses_math)\n",
    "# [5](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/resources/lecture-22-gradient-descent-downhill-to-a-minimum/)\n",
    "# [6](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent)\n",
    "# [7](https://www.youtube.com/watch?v=jc2IthslyzM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
