{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy\n",
    "\n",
    "# Entropy is a measure of unpredictability or randomness in data. It is used in data science, \n",
    "# information theory, and machine learning to help understand how uncertain or mixed up a \n",
    "# dataset is.\n",
    "\n",
    "# When entropy is high, it means there is a lot of unpredictability. \n",
    "# For example, if all outcomes are equally likely, like a shuffled deck of cards or rolling a \n",
    "# fair die, entropy is at its highest. \n",
    "\n",
    "# If there is only one certain outcome, entropy is zero. \n",
    "# In data, more randomness or a less predictable pattern means higher entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4325de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applications in ML:\n",
    "\n",
    "# Entropy is part of the cross-entropy loss function, which is important for classification tasks. \n",
    "# When training a model, cross-entropy loss measures the difference between the true labels and \n",
    "# the predicted probabilities. \n",
    "# A lower value means the predicted probabilities are close to the actual labels, indicating a \n",
    "# better model. \n",
    "\n",
    "# Entropy is also used in building decision trees, where splits are made to maximize information \n",
    "# gain, leading to simpler and more accurate trees. \n",
    "\n",
    "# Entropy also helps in selecting the most informative features and in detecting unusual data \n",
    "# points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shannon Entropy\n",
    "\n",
    "# Shannon Entropy measures the average uncertainty in a random variable.\n",
    "\n",
    "# For a discrete random variable with possible outcomes x1, x2, ..., xn, each with probability \n",
    "# p(xi), the Shannon entropy H(X) is calculated as:\n",
    "\n",
    "# H(X) = −∑ p(xi) log₂ p(xi)\n",
    "\n",
    "# Where:\n",
    "# H(X) is the entropy (in bits, when using base 2 logarithms).\n",
    "# p(xi) is the probability of outcome xi.\n",
    "\n",
    "# For a fair six-sided die, each side has probability 1/6, so the entropy is:\n",
    "\n",
    "# H = −∑(i=1to6) (1/6) log₂ (1/6)\n",
    "#   = −6 * (1/6 log₂ (1/6))\n",
    "#   = log₂ 6 ≈ 2.585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899ed4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy: 1.4488156357251847\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "data = np.array([1, 2, 3, 1, 2, 2, 1])\n",
    "\n",
    "# Calculate probabilities of unique values\n",
    "values, counts = np.unique(data, return_counts=True)\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Calculate Shannon entropy using base 2 log\n",
    "shannon_entropy = entropy(probabilities, base=2)\n",
    "print(\"Shannon Entropy:\", shannon_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5391dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy: 1.4488156357251847\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "def shannon_entropy(data):\n",
    "    counts = collections.Counter(data)\n",
    "    probabilities = [count / len(data) for count in counts.values()]\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities)\n",
    "    return entropy\n",
    "\n",
    "# Example data\n",
    "data = ['a', 'b', 'a', 'c', 'b', 'b', 'a']\n",
    "print(\"Shannon Entropy:\", shannon_entropy(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy\n",
    "\n",
    "# Cross-entropy is widely used in machine learning. \n",
    "\n",
    "# It measures the difference between two probability distributions, often the true labels(p) \n",
    "# and predicted probabilities(q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://www.ametsoc.org/ams/publications/author-information/formatting-and-manuscript-components/mathematical-formulas-units-and-time-and-date/)\n",
    "# [2](https://proofreadingpal.com/proofreading-pulse/essays/how-to-format-math-and-equations/)\n",
    "# [3](https://chec.engineering.cornell.edu/writing-numbers-and-equations/)\n",
    "# [4](https://www.overleaf.com/learn/latex/Mathematical_expressions)\n",
    "# [5](https://owl.purdue.edu/owl/research_and_citation/ieee_style/tables_figures_and_equations.html)\n",
    "# [6](https://web.cs.ucdavis.edu/~amenta/w10/writingman.pdf)\n",
    "# [7](https://style.mla.org/formatting-math-equations/)\n",
    "# [8](https://www.ams.org/publications/authors/AMS-StyleGuide-online.pdf)\n",
    "# [9](https://www.v7labs.com/blog/cross-entropy-loss-guide)\n",
    "# [10](https://365datascience.com/tutorials/machine-learning-tutorials/cross-entropy-loss/)\n",
    "# [11](https://www.geeksforgeeks.org/machine-learning/what-is-cross-entropy-loss-function/)\n",
    "# [12](https://www.machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "# [13](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "# [14](https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
