{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c7a90b",
   "metadata": {},
   "source": [
    "## Common loss functions\n",
    "\n",
    "A loss function quantifies how far a model’s predictions are from the actual outcomes. During model training, the goal is to minimize this loss to improve accuracy.\n",
    "\n",
    "Two of the most common loss functions in machine learning are **Mean Squared Error (MSE) for regression and Cross Entropy Loss for classification**. Both can be implemented manually in Python and used via popular libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26832905",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "Used for regression, where the task is to predict continuous values.\n",
    "\n",
    "MSE calculates the average of the squared differences between predicted values (ŷ) and actual values (y):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where,   \n",
    "\n",
    "    yi = actual value  \n",
    "    ŷi = predicted value  \n",
    "    n = number of data points  \n",
    "\n",
    "\n",
    "Squaring the errors penalizes larger errors more heavily, making MSE sensitive to outliers.\n",
    "\n",
    "It is smooth and differentiable, suitable for gradient-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186c8b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual MSE: 0.375\n"
     ]
    }
   ],
   "source": [
    "# Manual Python Implementation\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_actual, y_predicted):\n",
    "    return np.mean((y_actual - y_predicted)**2)\n",
    "\n",
    "# Example values\n",
    "y_actual = np.array([3, -0.5, 2, 7])\n",
    "y_predicted = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "mse = mean_squared_error(y_actual, y_predicted)\n",
    "print(\"Manual MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb63ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn MSE: 0.375\n"
     ]
    }
   ],
   "source": [
    "# Built-in Function Usage (Scikit-learn)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_sklearn = mean_squared_error(y_actual, y_predicted)\n",
    "print(\"Sklearn MSE:\", mse_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b34b0",
   "metadata": {},
   "source": [
    "#### Cross-Entropy Loss (Binary and Multi-class)\n",
    "\n",
    "Used in classification to measure the difference between predicted probability distributions and true labels.\n",
    "\n",
    "For binary classification (log loss), the formula is:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log (\\hat{y}_i) + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "For multi-class classification with k classes:\n",
    "\n",
    "    Cross-entropy = -(1/n) * Σ Σ yic * log(ŷic)\n",
    "\n",
    "This loss penalizes confident but wrong predictions more severely, encouraging well-calibrated probability estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7689a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Binary Cross-Entropy Loss: 0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "# Manual Python Implementation (Binary)\n",
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy_loss(y_actual, y_predicted):\n",
    "    epsilon = 1e-15  # to avoid log(0)\n",
    "    y_predicted = np.clip(y_predicted, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_actual * np.log(y_predicted) + (1 - y_actual) * np.log(1 - y_predicted))\n",
    "\n",
    "# Example values\n",
    "y_actual = np.array([1, 0, 1])\n",
    "y_predicted = np.array([0.9, 0.2, 0.4])\n",
    "\n",
    "log_loss = binary_cross_entropy_loss(y_actual, y_predicted)\n",
    "print(\"Manual Binary Cross-Entropy Loss:\", log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ed4108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Binary Cross-Entropy Log Loss: 0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "# Built-in Function Usage (Scikit-learn)\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss_sklearn = log_loss(y_actual, y_predicted)\n",
    "print(\"Sklearn Binary Cross-Entropy Log Loss:\", log_loss_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5bed3",
   "metadata": {},
   "source": [
    "**MSE Example Calculation**:  \n",
    "\n",
    "Given:  \n",
    "y = [1, 2, 3]  \n",
    "ŷ = [1.1, 1.9, 3.2]  \n",
    "\n",
    "Calculation:  \n",
    "MSE = (1/n) * Σ (yi - ŷi)^2  \n",
    "\n",
    "MSE = ((1 - 1.1)² + (2 - 1.9)² + (3 - 3.2)²) / 3  \n",
    "    = (0.01 + 0.01 + 0.04) / 3  \n",
    "    = 0.02  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4a7a2",
   "metadata": {},
   "source": [
    "**Cross Entropy/Log Loss Example Calculation**:  \n",
    "\n",
    "Given three samples:  \n",
    "\n",
    "Sample | Actual | Predicted Probability  \n",
    "1      | 1      | 0.9  \n",
    "2      | 0      | 0.2  \n",
    "3      | 1      | 0.4  \n",
    "\n",
    "Compute log loss:  \n",
    "\n",
    "Sample 1 = -(1 * log 0.9 + 0 * log(1 - 0.9)) = -log 0.9 = 0.105  \n",
    "Sample 2 = -(0 * log 0.2 + 1 * log(1 - 0.2)) = -log 0.8 = 0.223  \n",
    "Sample 3 = -(1 * log 0.4 + 0 * log(1 - 0.4)) = -log 0.4 = 0.916  \n",
    "\n",
    "Average = (0.105 + 0.223 + 0.916) / 3 = 0.414  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fccfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# [1](https://www.w3resource.com/machine-learning/tensorflow/python-tensorflow-building-and-training-exercise-6.php)\n",
    "# [2](https://www.digitalocean.com/community/tutorials/loss-functions-in-python)\n",
    "# [3](https://neptune.ai/blog/pytorch-loss-functions)\n",
    "# [4](https://spotintelligence.com/2023/09/25/loss-functions/)\n",
    "# [5](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "# [6](https://www.geeksforgeeks.org/python/python-mean-squared-error/)\n",
    "# [7](https://www.youtube.com/watch?v=tJpzKILW-Kg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
